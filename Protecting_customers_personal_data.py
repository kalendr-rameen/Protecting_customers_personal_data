#!/usr/bin/env python
# coding: utf-8

# <h1>Содержание<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"><li><span><a href="#1.-Загрузка-данных" data-toc-modified-id="1.-Загрузка-данных-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>1. Загрузка данных</a></span></li><li><span><a href="#Умножение-матриц" data-toc-modified-id="Умножение-матриц-2"><span class="toc-item-num">2&nbsp;&nbsp;</span><code>Умножение матриц</code></a></span><ul class="toc-item"><li><span><a href="#В-данном-задание-нужно-ответить-на-вопрос-о-том-что-если-умножить-обратимую-матрицу-на-матрицу-признаков,---Ответьте-на-вопрос-и-обоснуйте-решение." data-toc-modified-id="В-данном-задание-нужно-ответить-на-вопрос-о-том-что-если-умножить-обратимую-матрицу-на-матрицу-признаков,---Ответьте-на-вопрос-и-обоснуйте-решение.-2.1"><span class="toc-item-num">2.1&nbsp;&nbsp;</span>В данном задание нужно ответить на вопрос о том что если умножить обратимую матрицу на матрицу признаков,   Ответьте на вопрос и обоснуйте решение.</a></span></li><li><span><a href="#Признаки-умножают-на-обратимую-матрицу.-Изменится-ли-качество-линейной-регрессии?-(Её-можно-обучить-заново.)" data-toc-modified-id="Признаки-умножают-на-обратимую-матрицу.-Изменится-ли-качество-линейной-регрессии?-(Её-можно-обучить-заново.)-2.2"><span class="toc-item-num">2.2&nbsp;&nbsp;</span>Признаки умножают на обратимую матрицу. Изменится ли качество линейной регрессии? (Её можно обучить заново.)</a></span></li><li><span><a href="#a.-Изменится.-Приведите-примеры-матриц." data-toc-modified-id="a.-Изменится.-Приведите-примеры-матриц.-2.3"><span class="toc-item-num">2.3&nbsp;&nbsp;</span>a. Изменится. Приведите примеры матриц.</a></span></li><li><span><a href="#b.-Не-изменится.-Укажите,-как-связаны-параметры-линейной-регрессии-в-исходной-задаче-и-в-преобразованной." data-toc-modified-id="b.-Не-изменится.-Укажите,-как-связаны-параметры-линейной-регрессии-в-исходной-задаче-и-в-преобразованной.-2.4"><span class="toc-item-num">2.4&nbsp;&nbsp;</span>b. Не изменится. Укажите, как связаны параметры линейной регрессии в исходной задаче и в преобразованной.</a></span></li></ul></li><li><span><a href="#Алгоритм-преобразования" data-toc-modified-id="Алгоритм-преобразования-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Алгоритм преобразования</a></span></li><li><span><a href="#Проверка-алгоритма" data-toc-modified-id="Проверка-алгоритма-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>Проверка алгоритма</a></span></li><li><span><a href="#Чек-лист-проверки" data-toc-modified-id="Чек-лист-проверки-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>

# # Защита персональных данных клиентов

# Вам нужно защитить данные клиентов страховой компании «Хоть потоп». Разработайте такой метод преобразования данных, чтобы по ним было сложно восстановить персональную информацию. Обоснуйте корректность его работы.
# 
# Нужно защитить данные, чтобы при преобразовании качество моделей машинного обучения не ухудшилось. Подбирать наилучшую модель не требуется.

# ## 1. Загрузка данных

# In[1]:


import pandas as pd 
import numpy as np 
from sklearn.metrics import mean_squared_error
from numpy.linalg import inv
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score


# In[2]:


df = pd.read_csv('/datasets/insurance.csv')


# In[3]:


df


# In[4]:


df.info()


# In[5]:


df.describe()


# In[6]:


features = df.drop(columns='Страховые выплаты').values


# In[7]:


target = df["Страховые выплаты"]


# **Вывод**

# Итак что мы можем заметить, данные чистые т.е без NaN-ов, собственно как мне кажется больше здесь сильно не о чем говорить, приступим к следующим заданиям!

# ## `Умножение матриц`

# ### В данном задание нужно ответить на вопрос о том что если умножить обратимую матрицу на матрицу признаков,   Ответьте на вопрос и обоснуйте решение.
#    ### Признаки умножают на обратимую матрицу. Изменится ли качество линейной регрессии? (Её можно обучить заново.)
#    ### a. Изменится. Приведите примеры матриц.
#    ### b. Не изменится. Укажите, как связаны параметры линейной регрессии в исходной задаче и в преобразованной.

# Итак я затрудняюсь ответить на этот вопрос по причине того что не совсем понимаю, вывод формул которые мне подсказали мои однокогорчане) Итак распишу поподробнее. 
# 
# Чтобы решить данную задачу нам нужно воспользоваться след.формулами 
# 
# Предсказания:
# 
# $$
# a = Xw (1)
# $$
# 
# Задача обучения:
# 
# $$
# w = \arg\min_w MSE(Xw, y)(2)
# $$
# 
# Формула обучения:
# 
# $$
# w = (X^T X)^{-1} X^T y (3)
# $$
# 
# 
# Затем при подстановке формулы обучения в формулу предсказаний у нас получается след.формула 
# 
# $$
# a = X (X^T X)^{-1} X^T y (4)
# $$
# 
# Затем каким то невероятным образом эта формула преобразуется в 
# $$
# a = X (X^T X)^{-1} X^T y = X X^{-1}(X^T)^{-1}X^T y  = X X^{-1} y (5)
# $$
# 
# Т.е вот эта часть $(X^T X)^{-1} X^T$ преобразуется в $X^{-1}(X^T)^{-1}X^T = X^{-1}$
# 
# <!-- $$
# (X^T X)^{-1} X^T = X^{-1}(X^T)^{-1}X^T = X^{-1}
# $$ -->
# 
# Момент который мне не понятен это как мы раскрываем скобки, я весь день вчера потратил на это ничего годного так и не придумав, перечитал трэды в слаке но так и не додумался . Сослан Табуев указывает следующие свойства матриц которые должны помочь в решение данного вопроса а именно:
# 
# `1. Не коммутативность `
# $$
# A*B \not =  B*A
# $$
# 
# `2. Свойство транспонирования произведения матриц`
# 
# $$
# (A*B)^T = B^T*A^T
# $$
# 
# `3. Cвойство обратной матрицы от произведения матриц `
# $$ 
# (A*B)^{-1} = B^{-1} * A^{-1}
# $$

# $$
# w_P = ((XP)^T XP)^{-1} (XP)^T y = P^{-1} \cdot (X^T  X)^{-1} \cdot (P^T)^{-1} \cdot P^T \cdot X^T \cdot y  = P^{-1} \cdot (X^T  X)^{-1} \cdot y
# $$
# $$
# a_P = X_P w_P = X \cdot P \cdot P^{-1} \cdot (X^T  X)^{-1} \cdot X^T \cdot y = X \cdot (X^T  X)^{-1} \cdot X^T \cdot y 
# $$
# 
# Исходя из этого пол-ся что $a_p = a$, что и требовалось док-ть
# 
# 
# 
# </div>

# In[10]:


features_new = features @ A


# In[11]:


features_new


# In[12]:


pd.DataFrame(features_new)


# Для наглядности я решил проверить, может это я туплю и не могу открыть скобки в данном уравнение $(X^T X)^{-1} X^T = X^{-1}(X^T)^{-1}X^T$, но выскакивает ошибка(ниже в виде скриншота), что в очередной раз меня заводит в тупик по поводу того как происходит всё же преобразование, что у нас получается такая формула $X^{-1}(X^T)^{-1}X^T$ которая почему то не работает для наших данных

# Внизу эта формула $(X^T X)^{-1} X^T$

# In[13]:


np.linalg.inv(features.T.dot(features)).dot(features.T)


# Теперь внизу эта формула $X^{-1}(X^T)^{-1}X^T$

# In[14]:


#inv(features).dot(inv(features.T)).dot(features.T)


# Итак теперь посчитаем веса для новой и изначальной матрицы и посчитаем MSE в обеих случаях

# In[15]:


w = np.linalg.inv(features.T.dot(features)).dot(features.T).dot(target)


# In[16]:


w_new = np.linalg.inv(features_new.T.dot(features_new)).dot(features_new.T).dot(target)


# In[17]:


mean_squared_error(features @ w, target)


# In[18]:


mean_squared_error(features_new @ w_new, target)


# И собственно как можно заметить значения MSE для преобразованных данных и для обычных не изменяются

# Обозначения:
# 
# - $X$ — матрица признаков (нулевой столбец состоит из единиц)
# 
# - $y$ — вектор целевого признака
# 
# - $P$ — матрица, на которую умножаются признаки
# 
# - $w$ — вектор весов линейной регрессии (нулевой элемент равен сдвигу)

# Предсказания:
# 
# $$
# a = Xw
# $$
# 
# Задача обучения:
# 
# $$
# w = \arg\min_w MSE(Xw, y)
# $$
# 
# Формула обучения:
# 
# $$
# w = (X^T X)^{-1} X^T y
# $$

# **Ответ:** 
# 
# **Обоснование:** ...

# ## Алгоритм преобразования

# **Алгоритм**
# 
# Итак раз мы можем умножать на обратимую матрицу и получать корректные предсказания для линейной регресии, то становится понятно что можно таким образом шифровать изначальные данные. Поэтому алгоритм такой
# 
# 1. Создаём обратимую матрицу А
# 2. Находим новую матрицу признаков X_new = X.dot(A) и проверяем ее на обратимость 
# 3. Обучим модели на новых признаках и на старых (X и X_new)
# 4. Сравним значения R2 для обеих моделей

# **Обоснование**
# 
# Т.к из-за умножения на обратимую матрицу как должно быть доказано(но я до этого всё ещё не додумался к сожалению) значение предсказаний не меняется поэтому такой метод шифрования можно применять

# ## Проверка алгоритма

# In[19]:


model_orig = LinearRegression().fit(features, target)
model_changed = LinearRegression().fit(features_new, target)


# In[20]:


r2_score(target, model_changed.predict(features_new))


# In[21]:


r2_score(target, model_orig.predict(features))

